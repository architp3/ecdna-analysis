{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d769f50a-daa5-45a8-8d90-dc577349cf46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a59a2e2-c65a-4830-a05c-93aa3bdccc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab93fb4-cb8d-4c07-ad59-3760d02820f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38a4698-1025-44cd-b5ff-bb6cbbdd6f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff1f23098f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f29b11c-ed34-4e91-8813-40e4c6375c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c2ad43-6250-44a6-96e8-75bfc99be187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26172ef89c734493ab29a7b98d00433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc00ac1-915e-44b7-bbaa-9dc7c8a15c31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0f8180-dc92-4fe6-b8a7-1b9ef530538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(context, question, temperature, image_str, output, actual, filename='output.csv'):\n",
    "    # Check if the file already exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    # Open the file in append mode, create if it doesn't exist\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # If the file does not exist, write the headers\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Context', 'Question', 'Temperature', 'Image_Path', 'Output', 'Actual'])\n",
    "            file_exists = True\n",
    "        # Write the data row\n",
    "        writer.writerow([context, question, temperature, image_str, output, actual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13aab4af-8dea-4f3e-af88-5f29291983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truths(file_path):\n",
    "    gt_path = os.path.join(file_path, 'test_im_ec_quantification.csv')\n",
    "    df = pd.read_csv(gt_path)\n",
    "    return dict(zip(df['image name'], df['# of ec']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6d6fe-fcf2-471f-97b7-fa02e071fae0",
   "metadata": {},
   "source": [
    "#### example prompt strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56fdd03-8002-422e-b23e-f046e4d8893a",
   "metadata": {},
   "source": [
    "Chain-of-Thought Prompting (CoT):\n",
    "* Give example question and answer before real question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060ed84-a05f-4472-8d60-de0cb20c7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_context = \"ecDNA is usually smaller than chromosomes or nuclei\"\n",
    "\n",
    "question1 = \"Identify the count of ecDNA. Return a numerical answer only\"\n",
    "img1 = \"2244.png\"\n",
    "answer1 = \"0\"\n",
    "\n",
    "question2 = \"Identify the count of ecDNA. Return a numerical answer only\"\n",
    "img2 = \"949.png\"\n",
    "answer2 = \"2\"\n",
    "\n",
    "question = \"Identify the count of ecDNA. Return a numerical answer only\"\n",
    "img = \"1730.png\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
    "model.eval()\n",
    "full_image = os.path.join(file_path, img_path)\n",
    "img = Image.open(full_image).convert('RGB')\n",
    "\n",
    "img_tif = img_path.split(\".\")[0] + \".tif\"\n",
    "ground_truth = ground_truths.get(img_tif, 'Unknown')\n",
    "\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "\n",
    "res, context, _ = model.chat(\n",
    "    image=img,\n",
    "    msgs=msgs,\n",
    "    context=img_context,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=temperature\n",
    ")\n",
    "save_to_csv(img_context, question, temperature, img_path, res, ground_truth, filename + \".csv\")\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a54a8-5780-4f2f-a0f4-3eb42ab48c69",
   "metadata": {},
   "source": [
    "Self-Consistency Prompting:\n",
    "* Do multiple trials and aggregate results using a transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3dccc-48f4-44cc-aa20-7ed2741fc95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do the code multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066f317-4549-46cf-af32-af30fb818325",
   "metadata": {},
   "source": [
    "Zero-Shot and Few-Shot Prompting:\n",
    "* Zero-shot prompting uses no prior examples, relying solely on the model's pre-existing knowledge, while few-shot prompting provides a small number of examples to guide the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726c53f-4621-4e6d-8dee-0c471fab8f7c",
   "metadata": {},
   "source": [
    "Prompt Chaining:\n",
    "* Use a series of prompts that build on each other to guide the model through a multi-step process or complex reasoning sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf33293-3563-40d1-9902-e4a20db48468",
   "metadata": {},
   "source": [
    "Meta-Prompting:\n",
    "* Prompts that include instructions or goals about how to generate subsequent prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c209450-7708-459f-a50f-ac4967a6395c",
   "metadata": {},
   "source": [
    "#### THING TO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f484fc6-3ebc-4d05-8954-b8a129c3dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_img(file_path, ground_truths, img_path, img_context, question, temperature, filename):\n",
    "\n",
    "    # just make the code here have your model take in:\n",
    "    # image, context, question, and temp \n",
    "    # return the result as res\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
    "    model.eval()\n",
    "    full_image = os.path.join(file_path, img_path)\n",
    "    img = Image.open(full_image).convert('RGB')\n",
    "\n",
    "    img_tif = img_path.split(\".\")[0] + \".tif\"\n",
    "    ground_truth = ground_truths.get(img_tif, 'Unknown')\n",
    "    \n",
    "    msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "    \n",
    "    res, context, _ = model.chat(\n",
    "        image=img,\n",
    "        msgs=msgs,\n",
    "        context=img_context,\n",
    "        tokenizer=tokenizer,\n",
    "        sampling=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    save_to_csv(img_context, question, temperature, img_path, res, ground_truth, filename + \".csv\")\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088b4a4-f3a9-4f92-ac0c-f7b2f4b34049",
   "metadata": {},
   "source": [
    "#### workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c185b92-7d22-4e51-bac4-687eaf526e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/1189 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "Processing Images: 100%|██████████| 1189/1189 [20:36<00:00,  1.04s/it]\n",
      "Processing Images: 100%|██████████| 1189/1189 [17:50<00:00,  1.11it/s]\n",
      "Processing Images:  15%|█▍        | 177/1189 [02:57<12:15,  1.38it/s] "
     ]
    }
   ],
   "source": [
    "# vars to change, should be self explanatory, file_path is just test vs train\n",
    "# filename is where to save stuff\n",
    "\n",
    "context = 'ecDNA is usually smaller than chromosomes or nuclei'\n",
    "\n",
    "for i in range(5):\n",
    "    file_path = \"../test_im_data\"\n",
    "    temperature = 0.7\n",
    "    question = 'Identify the count of ecDNA. Return a numerical answer only'\n",
    "    filename = 'test'\n",
    "    \n",
    "    file_path_l = os.path.join(file_path, \"labels\")\n",
    "    \n",
    "    ground_truth = load_ground_truths(file_path)\n",
    "    \n",
    "    files = [f for f in os.listdir(file_path_l) if f.endswith('.png')]\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for img_str in tqdm(files, desc=\"Processing Images\"):\n",
    "        res = eval_img(file_path_l, ground_truth, img_str, context, question, temperature, filename)\n",
    "        results.append((img_str, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fb723-7006-41b4-8946-52f102eef296",
   "metadata": {},
   "source": [
    "### todo\n",
    "* n-shot learning with 0, 1, and say 10\n",
    "* graph results of different learning types\n",
    "* understand minicpm llm aspect\n",
    "* different context analysis\n",
    "* descriptions of ecdna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
