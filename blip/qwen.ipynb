{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\archi\\anaconda3\\envs\\asid\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\archi\\anaconda3\\envs\\asid\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\archi\\anaconda3\\envs\\asid\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BlipProcessor, BlipForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import gradio as gr\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\archi\\\\ecdna-analysis\\\\data\\\\ecSeg_dataset.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zipfile path containing data\n",
    "zipfile_path = os.path.expanduser(\"~\") + '\\\\ecdna-analysis\\\\data\\\\ecSeg_dataset.zip'\n",
    "zipfile_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tiff_to_png(tiff_path, png_path):\n",
    "    with Image.open(tiff_path) as img:\n",
    "        img.save(png_path, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_file_path = 'C:\\\\Users\\\\archi\\\\ecdna-analysis\\\\data\\\\ecSeg_dataset.zip'\n",
    "\n",
    "# # Folder inside the ZIP file that contains the images\n",
    "# folder_name = 'train_im/'\n",
    "# extract_folder = 'train_im_images'\n",
    "\n",
    "# # Image extensions you want to extract\n",
    "# image_extensions = ('.jpg', '.jpeg', '.png', '.tif', '.bmp')\n",
    "\n",
    "# # Open the ZIP file\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     all_files = zip_ref.namelist()\n",
    "    \n",
    "#     # Filter image files from the specified folder\n",
    "#     image_files = [f for f in all_files if f.find(folder_name) != -1]\n",
    "    \n",
    "#     for image_file in image_files:\n",
    "#         # Extract the file to the extraction folder\n",
    "#         zip_ref.extract(image_file, extract_folder)\n",
    "        \n",
    "#         # Check if the file is a TIFF image (ends with .tif)\n",
    "#         if image_file.lower().endswith('.tif'):\n",
    "#             # Construct the local path for the TIFF file and the destination PNG path\n",
    "#             extracted_tiff_path = os.path.join(extract_folder, image_file)\n",
    "#             png_path = os.path.join(extract_folder, f\"{os.path.splitext(image_file)[0]}.png\")\n",
    "            \n",
    "#             # Convert the TIFF to PNG\n",
    "#             convert_tiff_to_png(extracted_tiff_path, png_path)\n",
    "            \n",
    "#             # Optionally, remove the original TIFF file after conversion\n",
    "#             os.remove(extracted_tiff_path)\n",
    "#             print(f\"Converted {image_file} to PNG.\")\n",
    "\n",
    "#     print(f\"Images extracted and converted to PNG in: {extract_folder}\")\n",
    "    \n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/archi/ecdna-analysis/blip/train_im_images/ecSeg_dataset/train_im/2389.png'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_num = 2389\n",
    "img_path = f'C:/Users/archi/ecdna-analysis/blip/train_im_images/ecSeg_dataset/train_im/{str(img_num)}.png'\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image shows 4 blue-marked chromosomes.']\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"The image shown is a picture of a cell. How many chromosomes are in the image which are shown by the blue markers?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
